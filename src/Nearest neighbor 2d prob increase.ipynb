{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b9dbf3",
   "metadata": {},
   "source": [
    "### Testing \"local\" vs. \"global\" effect\n",
    "It is possible that the nearest neighbor being connected merely indicates that the presynaptic neuron has a high out-degree, which then increases the connection probabilities with _all_ other neurons. We call this the \"global\" morphology effect.\n",
    "Alternatively, there can be a spatially limited effect, where the connection probability is increased in a specific area around the nearest neighbor. We call this a \"local\" morphology effect. \n",
    "\n",
    "The total effect can be a mixture of both, and in this notebook we conduct further analyses to further separate them. \n",
    "\n",
    "To that end we conduct an analysis of connection probabilities not just against distance, but against horizontal and vertical offsets from the pre-/post-synaptic neuron. \n",
    "The global effect will have no specific spatial structure, i.e., the increase in connection probability if the nearest neighbor is connected should be the same at each offset.\n",
    "Additionally, we analyze also the connectivity of a **configuration model** fitted against the data. That is a model that shuffles the locations of connections, but preserves the in- and out-degrees of all neurons. As the global effect is simply based on some neurons having a higher degree than others, the configuration model will capture it completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619ac87",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We begin by importing relevant packages and setting up paths.\n",
    "\n",
    "### Customization options\n",
    "We have set up this notebook and the paths below for the analysis of the excitatory subgraph of the MICrONS connectome, version 1412.\n",
    "\n",
    "But you can use it to analyze other connectomes as well, as long as they are formatted in the .hdf5-based format of Connectome-Utilities.\n",
    "\n",
    "One such example is full_microns_PA.h5, which is created by the notebook \"Fit other models to microns\" and contains a preferential-attachment model fit to the microns data.\n",
    "\n",
    "### Caching the results\n",
    "This analysis is somewhat computationally expensive (~20 minutes for 50k neuron connectomes). Hence, we broke it up into two parts: First, we calculate a pandas.DataFrame that holds pre-digested data. Then we plot the data according to customizable specifications. \n",
    "\n",
    "The output of the first, expensive step is saved into a file, such that it does not have to be re-calculated: In future executions it is instead read from the file, unless you set the \"force_recalculation\" flag to True. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import conntility\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets, interactive\n",
    "\n",
    "# Location of the file holding the connectome information\n",
    "# Obtain the file at https://doi.org/10.5281/zenodo.16744240 (MICrONS data)\n",
    "# or at https://doi.org/10.5281/zenodo.16744766 (Potential connectivity of an SSCX model)\n",
    "fn_connectivity = \"microns_mm3_connectome_v1412.h5\"\n",
    "fn_connectivity = \"/Users/mwr/Documents/artefacts/connectomes/rat_struc_l23_with_controls.h5\"\n",
    "\n",
    "# Location of the file for storing/caching results of the compuations. If it does not exists, it will be created.\n",
    "# The file below already has the relevant results pre-calculted.\n",
    "fn_digest_dataframe = \"/Users/mwr/Documents/artefacts/connectomes/rat_struc_l23_digest.h5\"\n",
    "\n",
    "# If you set this flag to True, expensive calculations will be repeated even if the result already exists in the cache file above.\n",
    "# Note: If you set this to True you will have to update \"fn_digest_dataframe\" to a different, local path. This is because the \n",
    "# default location of \"fn_digest_dataframe\" is on a read-only file system.\n",
    "force_recalculation = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c90aa",
   "metadata": {},
   "source": [
    "## Selecting the connection matrix\n",
    "We read a connection matrix from an .h5 file. However, a file can contain more than one connection matrix. \n",
    "\n",
    "**Select the one to analyze from the dropdown.**\n",
    "\n",
    "If you are using the recommended file, then you can choose between \"condensed\" and \"full\". The \"full\" matrix represents the connectome as a multigraph, i.e., multiple synapses from one neuron to another are represented by multiple edges. In \"condensed\" only one edge is placed, but it will have a property \"count\" that lists the number of edges in the \"full\" connectome. \n",
    "\n",
    "Which of the two you select will make no difference, as we convert the multigraph version to a condensed view anyways.\n",
    "\n",
    "If you are using one of the files containing control connectomes, then you can select the instance of the control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379a57b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579edb755b9c4bf4aa2586be206fdad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select matrix', options=('connectivity/configuration_model', 'connectivity/data', 'conneâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File(fn_connectivity, \"r\") as h5:\n",
    "    contents = []\n",
    "    prefixes = list(h5.keys())\n",
    "    for prefix in prefixes:\n",
    "        [contents.append(prefix + \"/\" + _k) for _k in h5[prefix].keys()]\n",
    "\n",
    "sel_matrix = widgets.Dropdown(options=contents, index=0, description=\"Select matrix\")\n",
    "display(sel_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8022d",
   "metadata": {},
   "source": [
    "We load the selected connectivity matrix.\n",
    "If it is a multigraph, we convert it not to be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8955104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20037 nodes with 67124018 edges from connectivity/data\n"
     ]
    }
   ],
   "source": [
    "selected = str(sel_matrix.value)\n",
    "M = conntility.ConnectivityMatrix.from_h5(fn_connectivity, \n",
    "                                          prefix=sel_matrix.value.split(\"/\")[0],\n",
    "                                          group_name=sel_matrix.value.split(\"/\")[1])\n",
    "if M.is_multigraph:\n",
    "    M = M.compress()\n",
    "print(f\"Loaded {len(M)} nodes with {len(M.edges)} edges from {selected}\")\n",
    "\n",
    "col_y = \"y\"\n",
    "col_xz = [\"x\", \"z\"]\n",
    "# This flag indicates whether y indicates a \"depth\", i.e. is counted from the top of L1 (True), or a reverse depth, counted from the bottom of L6 (False).\n",
    "# This is relevatn purely for plotting purposes at the end.\n",
    "y_is_depth = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398aa2",
   "metadata": {},
   "source": [
    "### Generate spatial bins\n",
    "We generate spatial bins for the offset of neurons pairs along the y-axis, and their distance in the x/z-plane.\n",
    "For the y-bins we ensure that they are centered around 0.\n",
    "\n",
    "We found 50 um to be a good bin size, but you can update it. In that case, you will have to set the \"force_recalculate\" flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75efd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to adjust the bin size of the final plots here.\n",
    "bin_sz = 50.0 # um\n",
    "\n",
    "\n",
    "def make_spatial_bins(M_h, cols, bin_sz):\n",
    "    _data = M_h.vertices[cols]\n",
    "    delta = _data.max() - _data.min()\n",
    "\n",
    "    sz = numpy.sqrt((delta.values ** 2).sum())\n",
    "    if len(delta) == 1: # case 1d: negative and positive bins\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([-bins[:0:-1], bins])\n",
    "    else: # case 2d: Only positive bins, but exclude 0 dist\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([[0, 1E-12], bins[1:]])\n",
    "    return bins\n",
    "\n",
    "dbins_xz = make_spatial_bins(M, col_xz, bin_sz)\n",
    "binid_xz = numpy.arange(0, len(dbins_xz) + 1)\n",
    "\n",
    "dbins_y = make_spatial_bins(M, [col_y], bin_sz)\n",
    "binid_y = numpy.arange(0, len(dbins_y) + 1)\n",
    "\n",
    "bin_centers_y = 0.5 * (dbins_y[:-1] + dbins_y[1:])\n",
    "bin_centers_xz = 0.5 * (dbins_xz[1:-1] + dbins_xz[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee189816",
   "metadata": {},
   "source": [
    "## Find nearest neighbors and calculate connectivity with nearest neighbors\n",
    "\n",
    "We use KDTrees to quickly find nearest neigbors of all neurons.\n",
    "\n",
    "Then we calculate three sparse matrices:\n",
    "  - The first simply holds the number of edges between neurons i and j\n",
    "  - The second holds the number of edges between i and the nearest neigbor of j\n",
    "  - The third holds the number of edges between the nearest neigbbor of i and j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ebc4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "_coords = col_xz + [col_y]\n",
    "tree = KDTree(M.vertices[_coords].values)\n",
    "\n",
    "_, nn_id = tree.query(M.vertices[_coords], k=2)\n",
    "nn_id = nn_id[:, 1]  # nn_id[:, 0] is the original node, which has distance 0. nn_id[:, 1] is neighbor\n",
    "\n",
    "# Lookup from pre / post ids to number of edges\n",
    "pair_to_edge_count = M.edges.set_index(pandas.MultiIndex.from_frame(M._edge_indices))[\"count\"]\n",
    "\n",
    "# Edge counts i -> j\n",
    "edge_count_mat = M.matrix.tocsr()\n",
    "# Edge counts nn(i) -> j\n",
    "edge_count_nnpre_mat = edge_count_mat[nn_id]\n",
    "# Edge counts i -> nn(j)\n",
    "edge_count_nnpost_mat = edge_count_mat[:, nn_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59809d2",
   "metadata": {},
   "source": [
    "## Main calculation\n",
    "\n",
    "Here we calculate a DataFrame with the following structure:\n",
    "\n",
    "Each row represents a possible combination of:\n",
    "  - spatial bin of the offset in the xz plane of a pair of neurons (i - j)\n",
    "  - spatial bin of their offset along the y-axis\n",
    "  - number of edges (synapses) from i to j\n",
    "  - number of edges (synapses) from i to the nearest neighbor of j\n",
    "\n",
    "The first four columns list the values of these four properties. **a fifth column then counts the number of pairs of neurons for that combination**.\n",
    "\n",
    "Additionally, we calculate a second DataFrame that is very similar, except the fourth column represents instead the number of edges (synapses) from the nearest neighbor of i to j.\n",
    "\n",
    "\n",
    "For this calculation we have to consider the distances between all pairs of neurons. For large connectomes this can result in a very large matrix. To avoid running out of memory on weaker machines, we conduct the analysis in chunks of 2500 neurons. In the next cell, we define the function we run for each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041e1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the analysis for a given set of _rows_ of the connectivity matrix\n",
    "def for_pre_chunk(chunk_pre):\n",
    "    # Which offset bin the pairs fall into\n",
    "    Dxz = cdist(M.vertices.iloc[chunk_pre][col_xz], M.vertices[col_xz]) # PRE X POST\n",
    "    Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1. That is the one to exclude.\n",
    "\n",
    "    Dy = -M.vertices.iloc[chunk_pre][[col_y]].values + M.vertices[[col_y]].values.transpose() # PRE X POST\n",
    "    Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Values are post - pre, i.e. the delta y along the direction of connection\n",
    "    # Numer of edges i -> j\n",
    "    edge_count = edge_count_mat[chunk_pre].toarray().flatten()\n",
    "\n",
    "    # Number of edges nn(i) -> j\n",
    "    edge_count_nnpre = edge_count_nnpre_mat[chunk_pre].toarray().flatten()\n",
    "    # is nn(i) == j?\n",
    "    collision_pre = (nn_id[chunk_pre].reshape((-1, 1)) - numpy.arange(edge_count_mat.shape[1]).reshape((1, -1))) != 0\n",
    "    collision_pre = collision_pre.flatten()\n",
    "\n",
    "    # Number of edges i -> nn(j)\n",
    "    edge_count_nnpost = edge_count_nnpost_mat[chunk_pre].toarray().flatten()\n",
    "    # is i == nn(j)?\n",
    "    collision_post = (chunk_pre.reshape((-1, 1)) - nn_id.reshape((1, -1))) != 0\n",
    "    collision_post = collision_post.flatten()\n",
    "\n",
    "    # Count instances of each\n",
    "    ret_incoming = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten()[collision_pre],\n",
    "        \"y\": Dy.flatten()[collision_pre],\n",
    "        \"edges_pair\": edge_count[collision_pre],\n",
    "        \"edges_nn\": edge_count_nnpre[collision_pre],\n",
    "    }).value_counts()\n",
    "    \n",
    "    ret_outgoing = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten()[collision_post],\n",
    "        \"y\": Dy.flatten()[collision_post],\n",
    "        \"edges_pair\": edge_count[collision_post],\n",
    "        \"edges_nn\": edge_count_nnpost[collision_post],\n",
    "    }).value_counts()\n",
    "    return ret_incoming, ret_outgoing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66345460",
   "metadata": {},
   "source": [
    "### Try the cache first\n",
    "\n",
    "Here, we test whether the result can already be found in the cache (\"digest\") file. If so and \"force_recalculation\" is False, we load it.\n",
    "\n",
    "Otherwise, we iterate over chunks of neurons performing the costly calculation. This may take 20-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747c8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_exists = False\n",
    "if not force_recalculation:\n",
    "    if os.path.isfile(fn_digest_dataframe):\n",
    "        with h5py.File(fn_digest_dataframe, \"r\") as h5:\n",
    "            if selected + \"/incoming\" in h5 and selected + \"/outgoing\" in h5:\n",
    "                digest_exists = True\n",
    "\n",
    "if digest_exists:\n",
    "    full_df_incoming = pandas.read_hdf(fn_digest_dataframe, selected + \"/incoming\")\n",
    "    full_df_outgoing = pandas.read_hdf(fn_digest_dataframe, selected + \"/outgoing\")\n",
    "else:\n",
    "    chunk_sz = 2500\n",
    "    chunking = numpy.arange(0, len(M) + chunk_sz, chunk_sz)\n",
    "\n",
    "    chunk = numpy.arange(chunking[0], numpy.minimum(chunking[1], len(M)))\n",
    "    full_df_incoming, full_df_outgoing = for_pre_chunk(chunk)\n",
    "\n",
    "    for a, b in tqdm.tqdm(list(zip(chunking[1:-1], chunking[2:]))):\n",
    "        chunk = numpy.arange(a, numpy.minimum(b, len(M)))\n",
    "        new_df_in, new_df_out = for_pre_chunk(chunk)\n",
    "        full_df_incoming = full_df_incoming.add(new_df_in, fill_value=0)\n",
    "        full_df_outgoing = full_df_outgoing.add(new_df_out, fill_value=0)\n",
    "    \n",
    "    full_df_incoming = full_df_incoming.drop(-1, axis=0).reset_index()\n",
    "    full_df_outgoing = full_df_outgoing.drop(-1, axis=0).reset_index()\n",
    "\n",
    "\n",
    "    assert (full_df_incoming[[\"xz\", \"y\"]] >= 0).all().all()\n",
    "    assert (full_df_outgoing[[\"xz\", \"y\"]] >= 0).all().all()\n",
    "\n",
    "    assert (full_df_incoming[\"xz\"] < len(binid_xz)).all()\n",
    "    assert (full_df_incoming[\"y\"] < len(binid_y)).all()\n",
    "    assert (full_df_outgoing[\"xz\"] < len(binid_xz)).all()\n",
    "    assert (full_df_outgoing[\"y\"] < len(binid_y)).all()\n",
    "\n",
    "    full_df_incoming[\"xz\"] = bin_centers_xz[full_df_incoming[\"xz\"]]\n",
    "    full_df_incoming[\"y\"] = bin_centers_y[full_df_incoming[\"y\"]]\n",
    "\n",
    "    full_df_outgoing[\"xz\"] = bin_centers_xz[full_df_outgoing[\"xz\"]]\n",
    "    full_df_outgoing[\"y\"] = bin_centers_y[full_df_outgoing[\"y\"]]\n",
    "\n",
    "    full_df_incoming.to_hdf(fn_digest_dataframe, key=(selected + \"/incoming\"))\n",
    "    full_df_outgoing.to_hdf(fn_digest_dataframe, key=(selected + \"/outgoing\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4f14a",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "Here, we plot the results. As in the notebook \"microns morphology effect\", we plot the overall (prior) connection probabilies in spatial bins, and the (posterior) connection probability, conditional on the nearest neighbor of a neuron being connected.\n",
    "\n",
    "From the pre-digested representation of the data we created above, a plot can be rapidly created.\n",
    "\n",
    "We make the plot customizable:\n",
    "  - thresh_pair: By default, a pair of neurons is considered connected if there is at least 1 synapse between them. But you can increase this to require 2, 3, or more synapses to investigate the spatial structure of stronger connections\n",
    "  - thresh_nn: Similar to the above. This is the threshold of synapse count for considering the nearest neighbor connected.\n",
    "  - required_count: Minimum number of neuron pairs for a valid connection probability estimate in a spatial bin. It can be argued that a connection probability calculated from a single pair of neurons is meaningless. Increase this value to avoid that.\n",
    "  - clim_max: Adjust how tight the limits of the color bar are set.\n",
    "  - show_relative: If unchecked, then the raw difference of posterior and prior connection probability is plotted. Otherwise, their relative difference is plotted (Michaelson contrast, i.e., bounded between -1 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afbb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa1f28cb9434f2793bf06ee78df3800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='thresh_pair', max=10, min=1), IntSlider(value=1, descripâ€¦"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def p_prior_post_fun(df_in, thresh_pair=1, thresh_nn=1, required_count=1):\n",
    "    v_pair = df_in[\"edges_pair\"] >= thresh_pair\n",
    "    v_nn = df_in[\"edges_nn\"] >= thresh_nn\n",
    "\n",
    "    if df_in.loc[v_nn, \"count\"].sum() < required_count:\n",
    "        return pandas.Series({\n",
    "            \"prior\": numpy.nan,\n",
    "            \"posterior\": numpy.nan\n",
    "        })\n",
    "\n",
    "    prior = df_in.loc[v_pair, \"count\"].sum() / df_in[\"count\"].sum()\n",
    "    \n",
    "    df_in = df_in.loc[v_nn]\n",
    "    v_pair = v_pair[v_nn]\n",
    "    posterior = 0.0\n",
    "    if numpy.any(v_pair):\n",
    "        posterior = df_in.loc[v_pair][\"count\"].sum() / df_in[\"count\"].sum()\n",
    "    return pandas.Series({\n",
    "        \"prior\": prior,\n",
    "        \"posterior\": posterior\n",
    "    })\n",
    "\n",
    "def make_extent(df):\n",
    "    delta_xz = df.columns[-1] - df.columns[-2]\n",
    "    delta_y = df.index[-1] - df.index[-2]\n",
    "    \n",
    "    extent = [df.columns[0] - delta_xz/2, df.columns[-1] + delta_xz/2,\n",
    "             df.index[-1] + delta_y/2, df.index[0] - delta_y/2]\n",
    "    return extent\n",
    "\n",
    "def show_results(res_df_incoming, res_df_outgoing, clim=[0, 0.1],\n",
    "                 show_relative=False):\n",
    "    fig = plt.figure(figsize=(4, 6))\n",
    "\n",
    "    i = 1\n",
    "    for prob_type in [\"prior\", \"posterior\"]:\n",
    "        for df, df_str in zip([res_df_incoming, res_df_outgoing],\n",
    "                              [\"Incoming\", \"Outgoing\"]):\n",
    "            ax = fig.add_subplot(3, 2, i)\n",
    "            img = df[prob_type].sort_index().unstack(\"xz\")\n",
    "            pltimg = ax.imshow(img, extent=make_extent(img), clim=clim)\n",
    "            ax.set_frame_on(False)\n",
    "            ax.set_title(f\"{df_str} connections\", fontsize=10)\n",
    "            # dy = y(post) - y(pre). If y is depth then a values > 0 indicates a _downwards_ connection.\n",
    "            # Hence, for \"Incoming\" we want values > 0 towards the top of the plot. if y is not depth,\n",
    "            # then the other way around.\n",
    "            if y_is_depth == (df_str == \"Incoming\"):\n",
    "                ax.set_ylim(sorted(ax.get_ylim()))\n",
    "            ax.set_xticks([])\n",
    "            if numpy.mod(i, 2) == 0:\n",
    "                plt.colorbar(pltimg, label=f\"{prob_type} prob.\")\n",
    "                ax.set_yticks([])\n",
    "            i += 1\n",
    "\n",
    "    clim_diff = [-clim[1], clim[1]]\n",
    "    for df, df_str in zip([res_df_incoming, res_df_outgoing],\n",
    "                              [\"Incoming\", \"Outgoing\"]):\n",
    "        ax = fig.add_subplot(3, 2, i)\n",
    "        img = df[\"posterior\"].subtract(df[\"prior\"], fill_value=0)\n",
    "        if show_relative:\n",
    "            img = img.divide(df[\"prior\"].add(df[\"posterior\"], fill_value=0), fill_value=0)\n",
    "            clim_diff = [-1.0, 1.0]\n",
    "        img = img.sort_index().unstack(\"xz\")\n",
    "            \n",
    "        pltimg = ax.imshow(img, extent=make_extent(img), clim=clim_diff, cmap=\"coolwarm\")\n",
    "        ax.set_frame_on(False)\n",
    "        ax.set_title(f\"Difference\", fontsize=10)\n",
    "        if y_is_depth == (df_str == \"Incoming\"):\n",
    "            ax.set_ylim(sorted(ax.get_ylim()))\n",
    "        ax.set_xticks(ax.get_xticks()); ax.set_xticklabels(ax.get_xticks(), rotation=\"vertical\")\n",
    "        if numpy.mod(i, 2) == 0:\n",
    "            plt.colorbar(pltimg)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def interact_fun(thresh_pair, thresh_nn, required_count, clim_max, show_relative):\n",
    "    res_in = full_df_incoming.groupby([\"xz\", \"y\"]).apply(p_prior_post_fun,  include_groups=False,\n",
    "                                                    thresh_pair=thresh_pair,\n",
    "                                                    thresh_nn=thresh_nn,\n",
    "                                                    required_count=required_count)\n",
    "    res_out = full_df_outgoing.groupby([\"xz\", \"y\"]).apply(p_prior_post_fun,  include_groups=False,\n",
    "                                                    thresh_pair=thresh_pair,\n",
    "                                                    thresh_nn=thresh_nn,\n",
    "                                                    required_count=required_count)\n",
    "    show_results(res_in, res_out, clim=[0, clim_max], show_relative=show_relative)\n",
    "\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=10, value=1)\n",
    "sel_thresh_nn = widgets.IntSlider(min=1, max=10, value=1)\n",
    "sel_required_count = widgets.IntSlider(min=1, max=100, value=1)\n",
    "sel_clim_max = widgets.FloatSlider(min=0.01, max=1.0, value=0.1, step=0.01)\n",
    "sel_relative = widgets.Checkbox(value=False)\n",
    "\n",
    "interactive(interact_fun, thresh_pair=sel_thresh_pair, thresh_nn=sel_thresh_nn,\n",
    "            clim_max=sel_clim_max, required_count=sel_required_count, show_relative=sel_relative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
