{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5454729",
   "metadata": {},
   "source": [
    "# Microns morphology effect part 3\n",
    "\n",
    "This analysis continues the notebook titled \"Nearest neighbor 1d prob increase\" and \"Nearest neighbor 2d prob increase\". In those notebooks we demonstrated that in the MICrONS connectome connections are not formed statistically independently. Specifically: If a connection exists from neuron i to neuron j, then the probability that a connection exists from i to the nearest neighbor of j is increased. We also demonstrated that this can only partially be explained by a \"global\" effect based on long-tailed degree distributions. Instead, the effect has a complex spatial structure.\n",
    "\n",
    "**We suggest you first look at those two notebook if you have not already.**\n",
    "\n",
    "### More on the spatial structure of the morphology effect\n",
    "Here we investigate the spatial structure further.\n",
    "\n",
    "The previous analyses demonstrated dependence between a connection i -> j and i -> nearest neighbor of j. But what about other neurons around j? How far does the effect reach? Additionally, it is conceivable that innervation of one location makes innervation of another location **less** likely.\n",
    "\n",
    "We explore this in the following way. Instead of an overall mean, we consider the connection probability of *individual neurons* into given spatial bins. Then we consider the correlations of connection probabilities into pairs of spatial bins over individual neurons. That is, we ask: If a neuron has a high connection probability into a spatial bin at one location, is its connection probability into another bin increased or decreased? \n",
    "\n",
    "As in part 2, such an effect can be global, where an increase into all other bins is observed, and which can be explained by degree distributions; or local, with spatial structure and not captured by a configuration model.\n",
    "\n",
    "### Similar to part 2 - but different\n",
    "You will note that the structure of this notebook and its analyses is similar to part 2. But there are important differences. So please read the explanations and code carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1580ec",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We begin by importing relevant packages and setting up paths.\n",
    "\n",
    "### Customization options\n",
    "We have set up this notebook and the paths below for the analysis of the excitatory subgraph of the MICrONS connectome.\n",
    "\n",
    "But you can use it to analyze other connectomes as well, as long as they are formatted in the .hdf5-based format of Connectome-Utilities.\n",
    "\n",
    "One such example is full_microns_PA.h5, which is created by the notebook \"Fit other models to microns\" and contains a preferential-attachment model fit to the microns data.\n",
    "\n",
    "### Caching the results\n",
    "This analysis is somewhat computationally expensive (~20 minutes for 50k neuron connectomes). Hence, we broke it up into two parts: First, we calculate a pandas.DataFrame that holds pre-digested data. Then we plot the data according to customizable specifications. \n",
    "\n",
    "The output of the first, expensive step is saved into a file, such that it does not have to be re-calculated: In future executions it is instead read from the file, unless you set the \"force_recalculation\" flag to True. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import conntility\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets, interactive, Dropdown, SelectMultiple\n",
    "\n",
    "# Location of the file holding the connectome information\n",
    "# Obtain the file at https://doi.org/10.5281/zenodo.16744240\n",
    "# or at https://doi.org/10.5281/zenodo.16744766 (Potential connectivity of an SSCX model)\n",
    "fn_connectivity = \"microns_mm3_connectome_v1412.h5\"\n",
    "\n",
    "# Location of the file for storing/caching results of the compuations. If it does not exists, it will be created.\n",
    "# The file below already has the relevant results pre-calculted.\n",
    "fn_digest_dataframe = os.path.splitext(fn_connectivity)[0] + \"_digest.h5\"\n",
    "\n",
    "# If you set this flag to True, expensive calculations will be repeated even if the result already exists in the cache file above.\n",
    "# Note: If you set this to True you will have to update \"fn_digest_dataframe\" to a different, local path. This is because the \n",
    "# default location of \"fn_digest_dataframe\" is on a read-only file system.\n",
    "force_recalculation = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5adb59",
   "metadata": {},
   "source": [
    "## Selecting the connection matrix\n",
    "We read a connection matrix from an .h5 file. However, a file can contain more than one connection matrix. \n",
    "\n",
    "**Select the one to analyze from the dropdown.**\n",
    "\n",
    "If you are using the recommended file, then you can choose between \"condensed\" and \"full\". The \"full\" matrix represents the connectome as a multigraph, i.e., multiple synapses from one neuron to another are represented by multiple edges. In \"condensed\" only one edge is placed, but it will have a property \"count\" that lists the number of edges in the \"full\" connectome. \n",
    "\n",
    "Which of the two you select will make no difference, as we convert the multigraph version to a condensed view anyways.\n",
    "\n",
    "If you are using one of the files containing control connectomes, then you can select the instance of the control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341efaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f408bf5a2ac441dc844e6fd4d487b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select matrix', index=1, options=('connectivity/condensed', 'connectivity/full'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File(fn_connectivity, \"r\") as h5:\n",
    "    contents = []\n",
    "    prefixes = list(h5.keys())\n",
    "    for prefix in prefixes:\n",
    "        [contents.append(prefix + \"/\" + _k) for _k in h5[prefix].keys()]\n",
    "\n",
    "sel_matrix = widgets.Dropdown(options=contents, index=1, description=\"Select matrix\")\n",
    "display(sel_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718c693",
   "metadata": {},
   "source": [
    "We load the selected connectivity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffeb0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50948 nodes with 7463640 edges from connectivity/condensed\n"
     ]
    }
   ],
   "source": [
    "selected = str(sel_matrix.value)\n",
    "M = conntility.ConnectivityMatrix.from_h5(fn_connectivity, \n",
    "                                          prefix=sel_matrix.value.split(\"/\")[0],\n",
    "                                          group_name=sel_matrix.value.split(\"/\")[1])\n",
    "if M.is_multigraph:\n",
    "    M = M.compress()\n",
    "print(f\"Loaded {len(M)} nodes with {len(M.edges)} edges from {selected}\")\n",
    "\n",
    "col_y = \"y\"\n",
    "col_xz = [\"x\", \"z\"]\n",
    "# This flag indicates whether y indicates a \"depth\", i.e. is counted from the top of L1 (True), or a reverse depth, counted from the bottom of L6 (False).\n",
    "# This is relevatn purely for plotting purposes at the end.\n",
    "y_is_depth = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b474c63",
   "metadata": {},
   "source": [
    "## Select a trustworthy subset of neurons\n",
    "\n",
    "Some datasets (e.g., EM connectomes) require proofreading to provide trustworthy connectivity data.\n",
    "Here, we will build up two separate filters that determine whether we consider a neuron to have trustworthy outgoing and incoming connectivity data\n",
    "\n",
    "### Neurons to consider for outgoing connectivity\n",
    "\n",
    "In the following dropdown, please select a vertex (neuron) property that will serve as the basis for a filter. The filter will be used to determine neurons with trustworthy *outgoing* connectivity.\n",
    "\n",
    "**Hint**: For MICrONS EM connectome data, use \"status_axon\". The rationale is, that we consider a neurons outgoing connectivity only if the *axon* has been proofread. For data from computational models filtering is not required. Therefore, you can leave the dropdown at \"*no filtering*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4857582a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b9248aa5834ba28b2baf6418f04ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='OUTGOING', options=('No filtering', 'layer', 'mtype', 'pt_root_id', 'volume', 'pt_superv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_fltr = \"No filtering\"\n",
    "select_fltr_row = Dropdown(options=[no_fltr, *M.vertex_properties],\n",
    "                           description=\"OUTGOING\")\n",
    "display(select_fltr_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de3fa8",
   "metadata": {},
   "source": [
    "Next, select all possible values of the selected neuron property that indicate a *trustworthy* neuron, i.e., a neuron that we **do** want to consider for outgoing connectivity.\n",
    "\n",
    "**Hint**: For MICrONS and \"status_axon\", select \"t\"\n",
    "\n",
    "If \"No filtering\" was selected, nothing will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc83b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac281f77d71148dbb6b0d479b159150b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Valid values', options=('empty', 't', 'f'), value=())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_vals_row = None\n",
    "if select_fltr_row.value != no_fltr:\n",
    "    v = M.vertices[select_fltr_row.value].drop_duplicates()\n",
    "    if len(v) > 20:\n",
    "        raise ValueError(f\"Property {select_fltr_row.value} has too many unique values. Please select a categorical property!\")\n",
    "    select_vals_row = SelectMultiple(options=v, description=\"Valid values\")\n",
    "    display(select_vals_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c78c0a",
   "metadata": {},
   "source": [
    "### Neurons to consider for incoming connectivity\n",
    "\n",
    "We repeat the same filter setup process for the **incoming** connectivity. \n",
    "What is required to consider a neuron's incoming connectivity trustworthy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "753245f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93312b4172440b4a64f36c0b0bc6d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='INCOMING', options=('No filtering', 'layer', 'mtype', 'pt_root_id', 'volume', 'pt_superv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_fltr_col = Dropdown(options=[no_fltr, *M.vertex_properties],\n",
    "                           description=\"INCOMING\")\n",
    "display(select_fltr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85f07b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_vals_col = None\n",
    "if select_fltr_col.value != no_fltr:\n",
    "    v = M.vertices[select_fltr_col.value].drop_duplicates()\n",
    "    if len(v) > 20:\n",
    "        raise ValueError(f\"Property {select_fltr_col.value} has too many unique values. Please select a categorical property!\")\n",
    "    select_vals_col = SelectMultiple(options=v, description=\"Valid values\")\n",
    "    display(select_vals_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a11fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 1871 neurons for outgoing, 50948 neurons for incoming connectivity!\n"
     ]
    }
   ],
   "source": [
    "index_row = M.vertices.index\n",
    "if select_vals_row is not None:\n",
    "    index_row = index_row[M.vertices.loc[index_row, select_fltr_row.value].isin(select_vals_row.value)]\n",
    "\n",
    "index_col = M.vertices.index\n",
    "if select_vals_col is not None:\n",
    "    index_col = index_col[M.vertices.loc[index_col, select_fltr_col.value].isin(select_vals_col.values)]\n",
    "index_row = index_row.to_numpy(); index_col = index_col.to_numpy()\n",
    "\n",
    "print(f\"Considering {len(index_row)} neurons for outgoing, {len(index_col)} neurons for incoming connectivity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842f3ea",
   "metadata": {},
   "source": [
    "### Generate spatial bins\n",
    "We generate spatial bins for the offset of neurons pairs along the y-axis, and their distance in the x/z-plane.\n",
    "For the y-bins we ensure that they are centered around 0.\n",
    "\n",
    "We found 50 um to be a good bin size, but you can update it. In that case, you will have to set the \"force_recalculate\" flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f922a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to adjust the bin size of the final plots here.\n",
    "bin_sz = 50.0 # um\n",
    "\n",
    "\n",
    "def make_spatial_bins(M_h, cols, bin_sz):\n",
    "    _data = M_h.vertices[cols]\n",
    "    delta = _data.max() - _data.min()\n",
    "\n",
    "    sz = numpy.sqrt((delta.values ** 2).sum())\n",
    "    if len(delta) == 1: # case 1d: negative and positive bins\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([-bins[:0:-1], bins])\n",
    "    else: # case 2d: Only positive bins, but exclude 0 dist\n",
    "        bins = numpy.arange(0, (bin_sz * numpy.ceil(sz / bin_sz)) + bin_sz, bin_sz)\n",
    "        bins = numpy.hstack([[0, 1E-12], bins[1:]])\n",
    "    return bins\n",
    "\n",
    "dbins_xz = make_spatial_bins(M, col_xz, bin_sz)\n",
    "binid_xz = numpy.arange(0, len(dbins_xz) + 1)\n",
    "\n",
    "dbins_y = make_spatial_bins(M, [col_y], bin_sz)\n",
    "binid_y = numpy.arange(0, len(dbins_y) + 1)\n",
    "\n",
    "bin_centers_y = 0.5 * (dbins_y[:-1] + dbins_y[1:])\n",
    "bin_centers_xz = 0.5 * (dbins_xz[1:-1] + dbins_xz[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d312a78",
   "metadata": {},
   "source": [
    "## Main calculation\n",
    "\n",
    "This is where this notebook differs from \"microns morphology effect part 2\". Although we still break up the population into chunks and execute the analysis chunk by chunk.\n",
    "\n",
    "### Step 1\n",
    "Here, we calculate a DataFrame with the following structure:\n",
    "\n",
    "Each row represents a possible combination of:\n",
    "  - A numeric identifier of a presynaptic neuron (i)\n",
    "  - spatial bin of an offset from neuron i in the xz plane\n",
    "  - spatial bin of an offset along the y-axis from neuron i\n",
    "  - number of edges (synapses) from i to a neuron j in the indicated spatial bin\n",
    "\n",
    "The first four columns list the values of these four properties. **a fifth column then counts the number of pairs of neurons for that combination**.\n",
    "\n",
    "The difference to \"microns morphology effect part 2\" is that we keep the counts for individual presynaptic neurons separate, instead of summing them.\n",
    "\n",
    "Additionally, we calculate a second DataFrame where i instead indicates a postsynaptic neuron and the number of edges from j to i is counted.\n",
    "\n",
    "### Step 2\n",
    "We then use this DataFrame to calculate the following: For each pre- (post-) synaptic neuron the number of potential synaptic partners it has in each spatial bin. The connection probability of each neuron into each spatial bin. This connection probability is calculated separately for different minimum edge counts, i.e. if 1, 2, 3, ... edges is the minumum for considering a pair of neurons connected. This allows focusing on the structure of exceptionally strong connections.\n",
    "\n",
    "### Caching the results\n",
    "As before, if the result has already been calculated before, it is read from a file instead. Otherwise it is stored into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25b37c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = M.matrix.tocsr()\n",
    "\n",
    "def for_chunk(chunk, direction=\"outgoing\"):\n",
    "    # Which offset bin the pairs fall into\n",
    "    if direction == \"outgoing\": # means: from chunk to all neurons\n",
    "        chunk_pre = chunk\n",
    "        chunk_post = index_col\n",
    "        Dxz = cdist(M.vertices.loc[chunk_pre, col_xz], M.vertices.loc[chunk_post, col_xz]) # PRE X POST\n",
    "        Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1\n",
    "\n",
    "        Dy = -(M.vertices.loc[chunk_pre, [col_y]].values - M.vertices.loc[chunk_post, [col_y]].values.transpose()) # PRE X POST\n",
    "        # PRE - POST. If y_is_depth: Negative values -> downwards connection.\n",
    "        Dy = numpy.digitize(Dy, dbins_y) - 1\n",
    "        \n",
    "        j, i = numpy.meshgrid(chunk_post, chunk_pre) # chunk is i: presyn. => consider outgoing\n",
    "        node_id = i.flatten()\n",
    "        edge_count = mat[numpy.ix_(chunk_pre, chunk_post)].toarray().flatten()\n",
    "    elif direction == \"incoming\":\n",
    "        chunk_pre = index_row\n",
    "        chunk_post = chunk\n",
    "        Dxz = cdist(M.vertices.loc[chunk_pre, col_xz], M.vertices.loc[chunk_post, col_xz]) # PRE X POST\n",
    "        Dxz = numpy.digitize(Dxz, dbins_xz) - 2  # -2 means distance = 0 will be bin id -1\n",
    "\n",
    "        Dy = -(M.vertices.loc[chunk_pre, [col_y]].values - M.vertices.loc[chunk_post, [col_y]].values.transpose()) # PRE X POST\n",
    "        # PRE - POST. If y_is_depth: Negative values -> downwards connection.\n",
    "        Dy = numpy.digitize(Dy, dbins_y) - 1  # NOTE: Negative values -> upwards connection\n",
    "        \n",
    "        j, i = numpy.meshgrid(chunk_post, chunk_pre) # chunk is j: postsyn. => consider incoming\n",
    "        node_id = j.flatten()\n",
    "        edge_count = mat[numpy.ix_(chunk_pre, chunk_post)].toarray().flatten()\n",
    "    \n",
    "    # Count instances of each\n",
    "    ret = pandas.DataFrame({\n",
    "        \"xz\": Dxz.flatten(),\n",
    "        \"y\": Dy.flatten(),\n",
    "        \"edge_count\": edge_count,\n",
    "        \"node_id\": node_id,\n",
    "    }).value_counts()\n",
    "    ret.name = \"count\"\n",
    "    return ret\n",
    "\n",
    "def update_xz_y_bins(df):\n",
    "    cols = list(df.index.names)\n",
    "    df = df.reset_index()\n",
    "    df[\"xz\"] = bin_centers_xz[df[\"xz\"]]\n",
    "    df[\"y\"] = bin_centers_y[df[\"y\"]]\n",
    "    df = df.set_index(cols)\n",
    "    return df[\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e14b1b",
   "metadata": {},
   "source": [
    "### Try the cache first\n",
    "\n",
    "Here, we test whether the result can already be found in the cache (\"digest\") file. If so and \"force_recalculation\" is False, we load it.\n",
    "\n",
    "Otherwise, we iterate over chunks of neurons performing the costly calculation. This may take 20-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b24e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.10s/it]\n",
      "100%|██████████| 51/51 [00:04<00:00, 10.47it/s]\n",
      "/var/folders/py/wqmt8l2s5zb8fhrxbd_b258w0000gn/T/ipykernel_62205/1264264168.py:45: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  P_out = df_for_outgoing.reset_index().groupby([\"xz\", \"y\"]).apply(conn_prob_for_thresholds)\n",
      "/var/folders/py/wqmt8l2s5zb8fhrxbd_b258w0000gn/T/ipykernel_62205/1264264168.py:47: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  N_out = df_for_outgoing.reset_index().groupby([\"xz\", \"y\"]).apply(simply_count)\n",
      "/var/folders/py/wqmt8l2s5zb8fhrxbd_b258w0000gn/T/ipykernel_62205/1264264168.py:49: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  P_in = df_for_incoming.reset_index().groupby([\"xz\", \"y\"]).apply(conn_prob_for_thresholds)\n",
      "/var/folders/py/wqmt8l2s5zb8fhrxbd_b258w0000gn/T/ipykernel_62205/1264264168.py:51: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  N_in = df_for_incoming.reset_index().groupby([\"xz\", \"y\"]).apply(simply_count)\n"
     ]
    }
   ],
   "source": [
    "digest_exists = False\n",
    "if not force_recalculation:\n",
    "    if os.path.isfile(fn_digest_dataframe):\n",
    "        with h5py.File(fn_digest_dataframe, \"r\") as h5:\n",
    "            if selected + \"/P_in\" in h5 and \\\n",
    "                selected + \"/N_in\" in h5 and \\\n",
    "                selected + \"/P_out\" in h5 and \\\n",
    "                selected + \"/P_in\" in h5:\n",
    "                    digest_exists = True\n",
    "\n",
    "if digest_exists:\n",
    "    P_in = pandas.read_hdf(fn_digest_dataframe, selected + \"/P_in\")\n",
    "    P_out = pandas.read_hdf(fn_digest_dataframe, selected + \"/P_out\")\n",
    "    N_in = pandas.read_hdf(fn_digest_dataframe, selected + \"/N_in\")\n",
    "    N_out = pandas.read_hdf(fn_digest_dataframe, selected + \"/N_out\")\n",
    "else:\n",
    "    df_for_outgoing = []\n",
    "    df_for_incoming = []\n",
    "\n",
    "    chunk_sz = 1000\n",
    "\n",
    "    chunking = numpy.arange(0, len(index_row) + chunk_sz, chunk_sz)\n",
    "    for a, b in tqdm.tqdm(list(zip(chunking[:-1], chunking[1:]))):\n",
    "        chunk = index_row[a:b]\n",
    "        df_for_outgoing.append(for_chunk(chunk, direction=\"outgoing\"))\n",
    "    \n",
    "    chunking = numpy.arange(0, len(index_col) + chunk_sz, chunk_sz)\n",
    "    for a, b in tqdm.tqdm(list(zip(chunking[:-1], chunking[1:]))):\n",
    "        chunk = index_col[a:b]\n",
    "        df_for_incoming.append(for_chunk(chunk, direction=\"incoming\"))\n",
    "        \n",
    "    df_for_outgoing = pandas.concat(df_for_outgoing, axis=0).drop(-1, axis=0, level=\"xz\")\n",
    "    df_for_incoming = pandas.concat(df_for_incoming, axis=0).drop(-1, axis=0, level=\"xz\")\n",
    "\n",
    "    df_for_outgoing = update_xz_y_bins(df_for_outgoing)\n",
    "    df_for_incoming = update_xz_y_bins(df_for_incoming)\n",
    "\n",
    "    def conn_prob_for_thresholds(df_in):\n",
    "        tmp = df_in.pivot(index=\"edge_count\", columns=\"node_id\", values=\"count\").fillna(0)\n",
    "        return (tmp.loc[::-1].cumsum(axis=0) / tmp.sum(axis=0)).stack()\n",
    "\n",
    "    def simply_count(df_in):\n",
    "        return df_in.reset_index().groupby(\"node_id\")[\"count\"].sum()\n",
    "\n",
    "    P_out = df_for_outgoing.reset_index().groupby([\"xz\", \"y\"]).apply(conn_prob_for_thresholds)\n",
    "    P_out = P_out.reorder_levels([2, 0, 1, 3])\n",
    "    N_out = df_for_outgoing.reset_index().groupby([\"xz\", \"y\"]).apply(simply_count)\n",
    "\n",
    "    P_in = df_for_incoming.reset_index().groupby([\"xz\", \"y\"]).apply(conn_prob_for_thresholds)\n",
    "    P_in = P_in.reorder_levels([2, 0, 1, 3])\n",
    "    N_in = df_for_incoming.reset_index().groupby([\"xz\", \"y\"]).apply(simply_count)\n",
    "\n",
    "    P_in.to_hdf(fn_digest_dataframe, key=(selected + \"/P_in\"))\n",
    "    P_out.to_hdf(fn_digest_dataframe, key=(selected + \"/P_out\"))\n",
    "    N_in.to_hdf(fn_digest_dataframe, key=(selected + \"/N_in\"))\n",
    "    N_out.to_hdf(fn_digest_dataframe, key=(selected + \"/N_out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78542521",
   "metadata": {},
   "source": [
    "## Optional normalization\n",
    "\n",
    "The following cell is optional. Set the \"perform_normalization\" flag to True to execute it.\n",
    "\n",
    "The normalization subtracts from the connection probabilities of a neuron its mean connection probability over all spatial bins. This is related to the distinction between a \"global\" and a \"local\" effect of morphology on connectivity. The global effect is one that can be explained by long-tailed degree distributions of neurons: If connection probability into one spatial bin is high, this indicates that the neuron has a high out-degree. Hence, connection probabilities into all other bins will be globally increased, without strong spatial structure. Any spatial structure to the correlation results on top of that will be attributed to a local effect.\n",
    "\n",
    "Subtracting from each neuron its mean connection probability will remove the impact of the global effect and allows us to study the two types of effect separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08c42390",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_normalization = True\n",
    "\n",
    "if perform_normalization:\n",
    "    mn_P_out = P_out.groupby([\"edge_count\", \"node_id\"]).mean()\n",
    "    mn_P_out = mn_P_out[pandas.MultiIndex.from_frame(P_out.index.to_frame()[[\"edge_count\", \"node_id\"]])]\n",
    "    mn_P_out.index = P_out.index\n",
    "    mn_P_in = P_in.groupby([\"edge_count\", \"node_id\"]).mean()\n",
    "    mn_P_in = mn_P_in[pandas.MultiIndex.from_frame(P_in.index.to_frame()[[\"edge_count\", \"node_id\"]])]\n",
    "    mn_P_in.index = P_in.index\n",
    "\n",
    "    P_out = (P_out / mn_P_out).dropna()\n",
    "    P_in = (P_in / mn_P_in).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd572877",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "Here, we plot the results. \n",
    "From the pre-digested representation of the data we created above, a plot can be (relatively) rapidly created.\n",
    "\n",
    "We make the plot customizable. \n",
    "\n",
    "**Read these descriptions carefully to understand the plots**\n",
    "  - direction: Consider the effect on \"incoming\" or \"outgoing\" connectivity.\n",
    "  - required_count: Minimum number of neuron pairs for a valid connection probability estimate in a spatial bin. It can be argued that a connection probability calculated from a single pair of neurons is meaningless. Increase this value to avoid that.\n",
    "  - required_corr: Minimum number of data points for a valid estimate or Pearson correlation. It can be argued that correlations between 2 samples are meaningless. Increase to avoid this.\n",
    "  - thresh_pair: By default, a pair of neurons is considered connected if there is at least 1 synapse between them. But you can increase this to require 2, 3, or more synapses to investigate the spatial structure of stronger connections\n",
    "  - bin_xz and bin_y: Select one spatial bin using the two dropdowns. What will be displayed is the Pearson correlations of connection probabilities into the selected bin and connection probabilities into *all possible* bins. The location of the selected bin will be indicated by a blue \"x\" mark. The correlation with the selected bin will always be 1.0 by definition.\n",
    "  - clim_max: Adjust how tight the limits of the color bar are set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d087cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e5ce2d36cd40b1be335ed2e718dcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='direction', index=1, options=('incoming', 'outgoing'), value='outg…"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_extent(df):\n",
    "    delta_xz = df.columns[-1] - df.columns[-2]\n",
    "    delta_y = df.index[-1] - df.index[-2]\n",
    "    \n",
    "    extent = [df.columns[0] - delta_xz/2, df.columns[-1] + delta_xz/2,\n",
    "             df.index[-1] + delta_y/2, df.index[0] - delta_y/2]\n",
    "    return extent\n",
    "\n",
    "def image_plot_function(direction, required_count, required_corr, thresh_pair, bin_xz, bin_y, clim_max):\n",
    "    if direction == \"incoming\":\n",
    "        N = N_in; P = P_in\n",
    "    elif direction == \"outgoing\":\n",
    "        N = N_out; P = P_out\n",
    "    \n",
    "    idx = N.index[N >= required_count]\n",
    "    src_bin = (bin_xz, bin_y)\n",
    "\n",
    "    fig = plt.figure(figsize=(2.5, 3.5))\n",
    "    ax = fig.gca()\n",
    "    ptgt = P[thresh_pair].reindex(idx, fill_value=0)\n",
    "    if src_bin not in ptgt:\n",
    "        ax.text(0.5, 0.5, \"Source bin empty!\", horizontalalignment=\"center\")\n",
    "        return\n",
    "    psrc = ptgt[src_bin]\n",
    "\n",
    "    def cc(data_in):\n",
    "        data_in = data_in.droplevel([0, 1])\n",
    "        idx = psrc.index.intersection(data_in.index)\n",
    "        if len(idx) > required_corr:\n",
    "            return numpy.corrcoef(data_in[idx], psrc[idx])[0, 1]\n",
    "        return numpy.nan\n",
    "    I = ptgt.groupby([\"xz\", \"y\"]).apply(cc, include_groups=False)\n",
    "    I = I.sort_index().unstack(\"xz\")\n",
    "    img = plt.imshow(I, clim=[-clim_max, clim_max],\n",
    "                     cmap=\"coolwarm\", extent=make_extent(I))\n",
    "    plt.colorbar(img, label=\"Pearson corr.\")\n",
    "    \n",
    "    if y_is_depth == (direction == \"incoming\"):\n",
    "        ax.set_ylim(sorted(ax.get_ylim()))\n",
    "    ax.plot(bin_xz, bin_y, marker='x', color=\"blue\")\n",
    "    ax.set_xlabel(\"hor. offset\")\n",
    "    ax.set_ylabel(\"vert. offset\")\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "sel_direction = widgets.Dropdown(options=[\"incoming\", \"outgoing\"], value=\"outgoing\")\n",
    "sel_required_count = widgets.IntSlider(min=1, max=250, value=50)\n",
    "sel_required_corr = widgets.IntSlider(min=2, max=100, value=10)\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=3, value=1)\n",
    "sel_xz = widgets.Dropdown(options=sorted(bin_centers_xz), index=1)\n",
    "sel_y = widgets.Dropdown(options=sorted(bin_centers_y), index=int(len(bin_centers_y)/2))\n",
    "sel_clim = widgets.FloatSlider(min=0.01, max=1.0, step=0.01, value=0.5)\n",
    "\n",
    "interactive(image_plot_function, direction=sel_direction, required_count=sel_required_count,\n",
    "            required_corr=sel_required_corr,\n",
    "            thresh_pair=sel_thresh_pair, bin_xz=sel_xz, bin_y=sel_y, clim_max=sel_clim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becf6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4cba3e5e864955912ee3cc9b18c47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='direction', index=1, options=('incoming', 'outgoing'), value='outg…"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def make_extent(df):\n",
    "    delta_xz = df.columns[-1] - df.columns[-2]\n",
    "    delta_y = df.index[-1] - df.index[-2]\n",
    "    \n",
    "    extent = [df.columns[0] - delta_xz/2, df.columns[-1] + delta_xz/2,\n",
    "             df.index[-1] + delta_y/2, df.index[0] - delta_y/2]\n",
    "    return extent\n",
    "\n",
    "def image_plot_function(direction, required_count, thresh_pair, bin_xz, bin_y):\n",
    "    clim_max = 3.7\n",
    "    if direction == \"incoming\":\n",
    "        N = N_in; P = P_in\n",
    "    elif direction == \"outgoing\":\n",
    "        N = N_out; P = P_out\n",
    "    \n",
    "    idx = N.index[N >= required_count]\n",
    "    src_bin = (bin_xz, bin_y)\n",
    "\n",
    "    fig = plt.figure(figsize=(2.5, 3.5))\n",
    "    ax = fig.gca()\n",
    "    ptgt = P[thresh_pair].reindex(idx, fill_value=0)\n",
    "    if src_bin not in ptgt:\n",
    "        ax.text(0.5, 0.5, \"Source bin empty!\", horizontalalignment=\"center\")\n",
    "        return\n",
    "    psrc = ptgt[src_bin]\n",
    "\n",
    "    def cc(data_in):\n",
    "        data_in = data_in.droplevel([0, 1])\n",
    "        idx = psrc.index.intersection(data_in.index)\n",
    "        return numpy.log10(len(idx))\n",
    "    I = ptgt.groupby([\"xz\", \"y\"]).apply(cc, include_groups=False)\n",
    "    I = I.sort_index().unstack(\"xz\")\n",
    "    img = plt.imshow(I, clim=[0, clim_max],\n",
    "                     cmap=\"rainbow\", extent=make_extent(I))\n",
    "    plt.colorbar(img, label=\"log10(num. neurons)\")\n",
    "    \n",
    "    if y_is_depth == (direction == \"outgoing\"):\n",
    "        ax.set_ylim(sorted(ax.get_ylim()))\n",
    "    ax.plot(bin_xz, bin_y, marker='x', color=\"blue\")\n",
    "    ax.set_xlabel(\"hor. offset\")\n",
    "    ax.set_ylabel(\"vert. offset\")\n",
    "\n",
    "sel_direction = widgets.Dropdown(options=[\"incoming\", \"outgoing\"], value=\"outgoing\")\n",
    "sel_required_count = widgets.IntSlider(min=1, max=250, value=50)\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=3, value=1)\n",
    "sel_xz = widgets.Dropdown(options=sorted(bin_centers_xz), index=1)\n",
    "sel_y = widgets.Dropdown(options=sorted(bin_centers_y), index=int(len(bin_centers_y)/2))\n",
    "\n",
    "interactive(image_plot_function, direction=sel_direction, required_count=sel_required_count,\n",
    "            thresh_pair=sel_thresh_pair, bin_xz=sel_xz, bin_y=sel_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "167bbd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d29a4765041c4a86778b9f430ffd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='direction', index=1, options=('incoming', 'outgoing'), value='outg…"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def make_extent(df):\n",
    "    delta_xz = df.columns[-1] - df.columns[-2]\n",
    "    delta_y = df.index[-1] - df.index[-2]\n",
    "    \n",
    "    extent = [df.columns[0] - delta_xz/2, df.columns[-1] + delta_xz/2,\n",
    "             df.index[-1] + delta_y/2, df.index[0] - delta_y/2]\n",
    "    return extent\n",
    "\n",
    "def image_plot_function(direction, required_count, thresh_pair, bin_xz, bin_y):\n",
    "    required_corr = 2\n",
    "    clim_max = 10\n",
    "    if direction == \"incoming\":\n",
    "        N = N_in; P = P_in\n",
    "    elif direction == \"outgoing\":\n",
    "        N = N_out; P = P_out\n",
    "    \n",
    "    idx = N.index[N >= required_count]\n",
    "    src_bin = (bin_xz, bin_y)\n",
    "\n",
    "    fig = plt.figure(figsize=(2.5, 3.5))\n",
    "    ax = fig.gca()\n",
    "    ptgt = P[thresh_pair].reindex(idx, fill_value=0)\n",
    "    if src_bin not in ptgt:\n",
    "        ax.text(0.5, 0.5, \"Source bin empty!\", horizontalalignment=\"center\")\n",
    "        return\n",
    "    psrc = ptgt[src_bin]\n",
    "\n",
    "    def cc(data_in):\n",
    "        data_in = data_in.droplevel([0, 1])\n",
    "        idx = psrc.index.intersection(data_in.index)\n",
    "        if len(idx) > required_corr:\n",
    "            return numpy.maximum(numpy.log10(pearsonr(data_in[idx], psrc[idx]).pvalue), -10)\n",
    "        return numpy.nan\n",
    "    I = ptgt.groupby([\"xz\", \"y\"]).apply(cc, include_groups=False)\n",
    "    I = I.sort_index().unstack(\"xz\")\n",
    "    level = numpy.log10(0.05 / (~I.isna()).sum().sum())\n",
    "    img = plt.imshow(I, clim=[-clim_max, 0],\n",
    "                     cmap=\"rainbow\", extent=make_extent(I))\n",
    "    plt.contour(I.to_numpy()[::-1], levels=[level], colors=[\"white\"], extent=make_extent(I), antialiased=False)\n",
    "    plt.colorbar(img, label=\"Pearson corr.\")\n",
    "    # dy = y(post) - y(pre). If y is depth then a values > 0 indicates a _downwards_ connection.\n",
    "    # Hence, for \"Incoming\" we want values > 0 towards the top of the plot. if y is not depth,\n",
    "    # then the other way around.\n",
    "    if y_is_depth == (direction == \"outgoing\"): # This is the opposite of part 2. I know. I apologize. But this is correct.\n",
    "        ax.set_ylim(sorted(ax.get_ylim()))\n",
    "    ax.plot(bin_xz, bin_y, marker='x', color=\"blue\")\n",
    "    ax.set_xlabel(\"hor. offset\")\n",
    "    ax.set_ylabel(\"vert. offset\")\n",
    "\n",
    "sel_direction = widgets.Dropdown(options=[\"incoming\", \"outgoing\"], value=\"outgoing\")\n",
    "sel_required_count = widgets.IntSlider(min=1, max=250, value=50)\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=3, value=1)\n",
    "sel_xz = widgets.Dropdown(options=sorted(bin_centers_xz), index=1)\n",
    "sel_y = widgets.Dropdown(options=sorted(bin_centers_y), index=int(len(bin_centers_y)/2))\n",
    "\n",
    "interactive(image_plot_function, direction=sel_direction, required_count=sel_required_count,\n",
    "            thresh_pair=sel_thresh_pair, bin_xz=sel_xz, bin_y=sel_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c5a60",
   "metadata": {},
   "source": [
    "## Cluster structure of correlations\n",
    "\n",
    "Here, we perform one last analysis. We cluster the structure of correlations of spatial bins and depict the results. This reveals which spatial bins tend to be innervated together!\n",
    "\n",
    "**Options**\n",
    "  - direction: Consider the effect on \"incoming\" or \"outgoing\" connectivity.\n",
    "  - required_count: Minimum number of neuron pairs for a valid connection probability estimate in a spatial bin. It can be argued that a connection probability calculated from a single pair of neurons is meaningless. Increase this value to avoid that.\n",
    "  - thresh_pair: By default, a pair of neurons is considered connected if there is at least 1 synapse between them. But you can increase this to require 2, 3, or more synapses to investigate the spatial structure of stronger connections\n",
    "  - louvain_res: Value of the resolution parameter of Louvain clustering.\n",
    "  - min_cluster_sz: Cluster that comprise fewer than that number of spatial bins will be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f681b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b707ac0de4004eca809a1cfaa2faa0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Calculating...', max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728dd4b88fb64d1394a5cb79a7e7d228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='...', description='Modularity')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776686613baa4d1a932235e5616666d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='direction', index=1, options=('incoming', 'outgoing'), value='outg…"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sknetwork.clustering import Louvain\n",
    "from sknetwork.clustering import get_modularity\n",
    "from scipy import stats\n",
    "\n",
    "progress = widgets.IntProgress(min=0, max=10, value=0, description=\"Calculating...\")\n",
    "modularity_display = widgets.HTML(\n",
    "    value=\"...\",\n",
    "    description='Modularity',\n",
    ")\n",
    "\n",
    "def cluster_and_show(direction, required_count, thresh_pair, louvain_res, min_cluster_sz):\n",
    "    progress.value = 0\n",
    "    if direction == \"incoming\":\n",
    "        N = N_in; P = P_in\n",
    "    elif direction == \"outgoing\":\n",
    "        N = N_out; P = P_out\n",
    "\n",
    "    fig = plt.figure(figsize=(2.5, 3.5))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    idx = N.index[N >= required_count]\n",
    "    ptgt = P[thresh_pair].reindex(idx, fill_value=0)\n",
    "    progress.value = 2\n",
    "\n",
    "    cc = ptgt.sort_index().unstack(\"node_id\").transpose().corr().fillna(0)\n",
    "    cc[cc < 0] = 0\n",
    "    progress.value = 7\n",
    "\n",
    "    res = Louvain(louvain_res).fit_predict(cc.values)\n",
    "    clst = pandas.Series(res, index=cc.index)\n",
    "\n",
    "    vc = clst.value_counts()\n",
    "    invalid = vc.index[vc < min_cluster_sz]\n",
    "    clst[clst.isin(invalid)] = numpy.nan\n",
    "    I = clst.unstack(\"xz\")\n",
    "    progress.value = 9\n",
    "\n",
    "    ax.imshow(I, extent=make_extent(I), cmap=\"Set3\")\n",
    "    ax.set_xlabel(\"hor. offset\")\n",
    "    ax.set_ylabel(\"vert. offset\")\n",
    "    ax.set_frame_on(False)\n",
    "    if y_is_depth == (direction == \"incoming\"): # This is the opposite of part 2. I know. I apologize. But this is correct.\n",
    "        ax.set_ylim(sorted(ax.get_ylim()))\n",
    "\n",
    "    modularity_display.value = f\"<b>{get_modularity(cc.values, res, resolution=louvain_res)}</b>\"\n",
    "    progress.value = 10\n",
    "\n",
    "sel_direction = widgets.Dropdown(options=[\"incoming\", \"outgoing\"], value=\"outgoing\")\n",
    "sel_required_count = widgets.IntSlider(min=1, max=250, value=50, behavior=\"tap\")\n",
    "sel_thresh_pair = widgets.IntSlider(min=1, max=3, value=1, behavior=\"tap\")\n",
    "sel_louvain = widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=1.0, behavior=\"tap\")\n",
    "sel_min_sz = widgets.IntSlider(min=1, max=200, value=20, behavior=\"tap\")\n",
    "\n",
    "display(progress)\n",
    "display(modularity_display)\n",
    "interactive(cluster_and_show, direction=sel_direction, required_count=sel_required_count,\n",
    "            thresh_pair=sel_thresh_pair, louvain_res=sel_louvain, min_cluster_sz=sel_min_sz)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
